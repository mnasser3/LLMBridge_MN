{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/mn/Desktop/BridgeAthletics/Proj1/DataPreProcessing')\n",
    "from Functions.DataSetManipulation import json_read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/mn/Desktop/BridgeAthletics/config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    api_key = config[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatgpt(prompt, client, model=\"gpt-3.5-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ CHOOSE APPROPRIATE DATA SET DIRECTORY #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/Users/mn/Desktop/BridgeAthletics/Dataset2_allparams/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ REMOVING USELESS BLOCK NAMES ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = dir+'initialdataset.json'\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in json_data[:5]:\n",
    "    block_name = entry[\"input\"]\n",
    "    prompt = f\"Does the workout title '{block_name}' clearly describe the focus or purpose of the workout? Reply only with 'yes' or 'no'.\"\n",
    "    print(prompt)\n",
    "    print(\"\\nOutput:\")\n",
    "    print(\">>\", run_chatgpt(prompt, client))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, entry in tqdm(enumerate(json_data), total=len(json_data)):\n",
    "    block_name = entry[\"input\"]\n",
    "    prompt = f\"Does the workout title '{block_name}' clearly describe the focus or purpose of the workout? Reply only with 'yes' or 'no'.\"\n",
    "    json_data[i][\"useful\"] = run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "for i in range(0,len(json_data)):\n",
    "    if (json_data[i][\"useful\"]!=\"Yes\") and (json_data[i][\"useful\"]!=\"No\"):\n",
    "        print(json_data[i][\"useful\"])\n",
    "    if json_data[i][\"useful\"]==\"No\":\n",
    "        n=n+1\n",
    "\n",
    "print(\"no:\",n,\"yes:\",len(json_data)-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############RUN ONCE##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_json_file = dir+\"dataset-usefulYesNo.json\"\n",
    "print(new_json_file)\n",
    "if os.path.exists(new_json_file):\n",
    "    m=input(\"are you sure you want to overwrite file? reply with 'yes' or 'no'\")\n",
    "    if m.lower()=='yes':\n",
    "        with open(new_json_file, \"w\") as file:\n",
    "            json.dump(json_data, file, indent=1)  \n",
    "else:\n",
    "    with open(new_json_file, \"w\") as file:\n",
    "        json.dump(json_data, file, indent=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = dir+\"dataset-usefulYesNo.json\"\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "print(len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = [item for item in json_data if item[\"useful\"] == \"Yes\"]\n",
    "print(len(filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############RUN ONCE##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_json_file = json_file.replace(\"-usefulYesNo.json\", \"-Yes.json\")\n",
    "with open(new_json_file, \"w\") as file:\n",
    "    json.dump(filtered_data, file, indent=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### EXTRACTING KEY INFO FROM BLOCK NAMES #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = dir+\"dataset-Yes.json\"\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "print(len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in json_data[:5]:\n",
    "    block_name = entry[\"input\"]\n",
    "    prompt = f\"For the workout title: '{block_name}', extract only the key phrases that describe the focus or type of the workout, expanding any abbreviations and removing any numerical details and session identifiers like 'day' or 'week' or 'min'. Reply only with the title.\"\n",
    "    print(prompt)\n",
    "    print(\"\\nOutput:\")\n",
    "    print(\">>\", run_chatgpt(prompt, client))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, entry in tqdm(enumerate(json_data), total=len(json_data)):\n",
    "    block_name = entry[\"input\"]\n",
    "    prompt = f\"For the workout title: '{block_name}', extract only the key words that describe the focus or type of the workout, expanding any abbreviations and removing any numerical details and session identifiers like dates, 'level', 'phase', 'day' or 'week'. Reply only with the title.\"\n",
    "    json_data[i][\"new_input\"] = run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = dir+\"dataset_gpt_prefinal.json\"\n",
    "if os.path.exists(json_file):\n",
    "    m=input(\"are you sure you want to overwrite file? reply with 'yes' or 'no'\")\n",
    "    if m.lower()=='yes':\n",
    "        with open(json_file, \"w\") as file:\n",
    "            json.dump(json_data, file, indent=1)\n",
    "else:\n",
    "    with open(json_file, \"w\") as file:\n",
    "        json.dump(json_data, file, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### DATA CLEANING: REMOVING REMAINING BAD ENTRIES ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def contains_keywords_or_number(s):\n",
    "    keywords = [\"title\", \"week\", \"day\", \"phase\",\"min\",\"dirty\"]\n",
    "    # Check for keywords\n",
    "    if any(keyword in s.lower() for keyword in keywords):\n",
    "        return True\n",
    "    # Check for numbers\n",
    "    if re.search(r'\\d', s):\n",
    "        return True\n",
    "    if s.lower()==\"block\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "print(len(json_data),json_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_arr=[]\n",
    "for i in range(len(json_data)):\n",
    "    if contains_keywords_or_number(json_data[i][\"new_input\"]):\n",
    "        print (i,json_data[i][\"new_input\"])\n",
    "        inc_arr.append(i)\n",
    "print([(i,j) for i,j in enumerate((inc_arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, entry in tqdm(enumerate(inc_arr),total=len(inc_arr)):\n",
    "    block_name = json_data[entry][\"new_input\"]\n",
    "    prompt = f\"For the workout title: '{block_name}', extract only the key words that describe the focus or type of the workout, session information like dates or 'level' or 'minutes' or 'phase'. Reply only with the title.\"\n",
    "    json_data[entry][\"new_input\"] = run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_arr=[]\n",
    "for i in range(len(json_data)):\n",
    "    if contains_keywords_or_number(json_data[i][\"new_input\"]):\n",
    "        print (i,json_data[i][\"new_input\"])\n",
    "        inc_arr.append(i)\n",
    "print([(i,j) for i,j in enumerate((inc_arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove=[2319,2177,2146,1504,1469,444,510,588,619]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN ONCE\n",
    "for i in sorted(inc_arr, reverse=True):\n",
    "    if i in to_remove:\n",
    "        del json_data[i]\n",
    "# print(json_data[1500])\n",
    "# print(len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in json_data:\n",
    "    item['input'] = item['new_input']\n",
    "    del item['new_input']\n",
    "    del item['useful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN CELL TWICE\n",
    "filtered_json_data = []\n",
    "\n",
    "for i in range(len(json_data)):\n",
    "    # Find isolated 'A', 'B', or 'C' in the 'input' field\n",
    "    matches = re.findall(r'\\b[A-C]\\b', json_data[i]['input'])\n",
    "    if matches:\n",
    "        if json_data[i]['input'][-2:] in [' A',' B',' C', ' D', ' E']:\n",
    "            json_data[i]['input']=json_data[i]['input'][:len(json_data[i]['input'])-2]\n",
    "        print(json_data[i]['input'])\n",
    "    if not matches:\n",
    "        filtered_json_data.append(json_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_json_data = filtered_json_data\n",
    "del filtered_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(f_json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(f_json_data) - 1, -1, -1):\n",
    "    if f_json_data[i]['input'].lower().startswith('block') or f_json_data[i]['input'].lower().startswith('test'):\n",
    "        del f_json_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(f_json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in f_json_data:\n",
    "#     print(i['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = dir+\"datasetprefinal.json\"\n",
    "if os.path.exists(json_file):\n",
    "    m=input(\"are you sure you want to overwrite file? reply with 'yes' or 'no'\")\n",
    "    if m.lower()=='yes':\n",
    "        with open(json_file, \"w\") as file:\n",
    "            json.dump(f_json_data, file, indent=1)\n",
    "else:\n",
    "    with open(json_file, \"w\") as file:\n",
    "        json.dump(f_json_data, file, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### REMOVING DUPLICATES ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = dir+\"datasetprefinal.json\"\n",
    "with open(json_file, \"r\") as file:\n",
    "    f_json_data = json.load(file)\n",
    "print(len(f_json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_caps_punct(input):\n",
    "    input = re.sub(r'[^\\w\\s]', '', input)\n",
    "    input = input.lower()\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_strings_cosine_similarity(string1, string2):\n",
    "    vectorizer = TfidfVectorizer(stop_words=None, analyzer='char', ngram_range=(1, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform([remove_caps_punct(string1).lower(), remove_caps_punct(string2).lower()])\n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return cos_sim_matrix[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_group_duplicates(json_data, threshold=0.9, key=\"input\"):\n",
    "    if key!=\"output\":\n",
    "        items = [remove_caps_punct(item[key]) for item in json_data]\n",
    "    else:\n",
    "        items = [remove_caps_punct(str(item[key])) for item in json_data]\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(stop_words=None, analyzer='char', ngram_range=(1, 3))\n",
    "    tfidf = vectorizer.fit_transform(items)\n",
    "\n",
    "    cos_sim_matrix = cosine_similarity(tfidf)\n",
    "\n",
    "    dup_groups = {}\n",
    "    unique_groups = {}\n",
    "    visited = set()\n",
    "\n",
    "    for i in range(len(cos_sim_matrix)):\n",
    "        if i in visited:\n",
    "            continue\n",
    "            \n",
    "        group = [i]\n",
    "        for j in range(i + 1, len(cos_sim_matrix)):\n",
    "            if cos_sim_matrix[i, j] > threshold:\n",
    "                group.append(j)\n",
    "                visited.add(j)\n",
    "        \n",
    "        if len(group) > 1:\n",
    "            dup_groups[i] = group\n",
    "            visited.add(i)\n",
    "\n",
    "        else:\n",
    "            unique_groups[i] = group\n",
    "            \n",
    "            \n",
    "    duplicate_groups = [[json_data[id] for id in group] for group in dup_groups.values()]\n",
    "    unique_groups = [[json_data[id] for id in group] for group in unique_groups.values()]\n",
    "\n",
    "    return duplicate_groups, unique_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_out_duplicates(json_data, key=\"output\"):\n",
    "    unique_out = []\n",
    "    unique_obj = []\n",
    "    output_dict = {}\n",
    "\n",
    "    for item in json_data:\n",
    "        output = item[key]\n",
    "        output_str=json.dumps(item[key])\n",
    "        input_block = item[\"input\"]\n",
    "\n",
    "        if output in unique_out:\n",
    "            similar = False\n",
    "            for existing_input in output_dict[output_str]:\n",
    "                if two_strings_cosine_similarity(input_block, existing_input) >= 0.9:\n",
    "                    similar = True\n",
    "                    break\n",
    "            if not similar:\n",
    "                unique_obj.append(item)\n",
    "                output_dict[output_str].append(input_block)\n",
    "        else:\n",
    "            unique_out.append(output)\n",
    "            unique_obj.append(item)\n",
    "            output_dict[output_str] = [input_block]\n",
    "\n",
    "    return unique_out, unique_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_groups, unique_groups = find_group_duplicates(f_json_data, threshold=1, key=\"output\")\n",
    "t=0\n",
    "if duplicate_groups:\n",
    "    print(\"Duplicate Groups Found:\")\n",
    "    for group_id, group in enumerate(duplicate_groups, 1):\n",
    "        # print(f\"Group {group_id}:\")\n",
    "        # print([group[i]['input'] for i in range (0,len(group))])\n",
    "        t=t+len(group)\n",
    "        g=group_id\n",
    "print(\"number of samples in duplicates:\",t,\"number of groups\",g)\n",
    "\n",
    "t=0\n",
    "if unique_groups:\n",
    "    print(\"Unique Groups Found:\")\n",
    "    for group_id, group in enumerate(unique_groups, 1):\n",
    "        # print(f\"Group {group_id}:\")\n",
    "        # print(group[0]['input'])\n",
    "        # print(len(group))\n",
    "        t=t+len(group)\n",
    "        g=group_id\n",
    "print(\"number of samples in uniques:\",t,\"number of groups\",g)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_outs,removed_dupli=find_out_duplicates(f_json_data)\n",
    "print(\"unique outputs/blocks:\",len(unique_outs),\"number of duplicate pairs:\",len(f_json_data)-len(removed_dupli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_json_data = removed_dupli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_json_file = dir+\"finaldataset.json\"\n",
    "with open(new_json_file, \"w\") as file:\n",
    "    json.dump(f_json_data, file, indent=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = dir+\"finaldataset.json\"\n",
    "with open(json_file, \"r\") as file:\n",
    "    final_json_data = json.load(file)\n",
    "print(len(final_json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL Removing blocks that are too long\n",
    "count=0\n",
    "for i in range(len(final_json_data) - 1, -1, -1):\n",
    "    if len(final_json_data[i][\"output\"]) >= 6:\n",
    "        del final_json_data[i]\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_json_file = dir+\"finaldataset_shortblocks.json\"\n",
    "with open(new_json_file, \"w\") as file:\n",
    "    json.dump(final_json_data, file, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = dir+\"finaldataset_shortblocks.json\"\n",
    "with open(json_file, \"r\") as file:\n",
    "    final_json_data = json.load(file)\n",
    "print(len(final_json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = dir+\"finaldataset_shortblocks.json\"\n",
    "with open(json_file, \"r\") as file:\n",
    "    final_json_data = json.load(file)\n",
    "print(len(final_json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## QUICK DATA ANALYSIS ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parameters_correctness(model_outputs_list, param_list):\n",
    "    required_keys = {'exercise', 'sets'}\n",
    "    c=0\n",
    "    for i, outer_list in enumerate(model_outputs_list):\n",
    "        for j, dictionary in enumerate(outer_list):\n",
    "            if not required_keys.issubset(dictionary.keys()):\n",
    "                print(f\"ERROR: Dictionary at index [{i}][{j}] must have at least the keys {required_keys}.\")\n",
    "                c=c+1\n",
    "                continue\n",
    "            \n",
    "            if not any(param in dictionary for param in param_list):\n",
    "                print(f\"ERROR: Dictionary at index [{i}][{j}] must have at least one of the keys from {param_list}.\")\n",
    "                c=c+1\n",
    "                continue\n",
    "            \n",
    "            allowed_keys = required_keys.union(param_list)\n",
    "            if not set(dictionary.keys()).issubset(allowed_keys):\n",
    "                print(f\"ERROR: Dictionary at index [{i}][{j}] must only have keys from {allowed_keys}.\")\n",
    "                c=c+1\n",
    "                continue\n",
    "    print(f\"inconsistency in keys for {c} out of {len(model_outputs_list)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_parameters_correctness(data_outputs,param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_param_frequency(model_outputs_list, param_list):\n",
    "    param_counts = {param: 0 for param in param_list}\n",
    "    \n",
    "    for outer_list in model_outputs_list:\n",
    "        for param in param_list:\n",
    "            if any(param in dictionary for dictionary in outer_list):\n",
    "                param_counts[param] += 1\n",
    "    \n",
    "    total_outer_lists = len(model_outputs_list)\n",
    "    param_percentages = {param: (count / total_outer_lists) * 100 for param, count in param_counts.items()}\n",
    "    \n",
    "    return param_counts, param_percentages\n",
    "\n",
    "\n",
    "param_counts, param_percentages = count_param_frequency(data_outputs, param_list)\n",
    "print(\"Parameter Counts:\", param_counts)\n",
    "print(\"Parameter Percentages:\", param_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO EDIT AS YOU INCREASE PARAMS\n",
    "param_list=['reps','time','distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_list_to_df(model_outputs_list,param_list):\n",
    "    flat_data = []\n",
    "    for i, outer_list in enumerate(model_outputs_list):\n",
    "        for d in outer_list:\n",
    "            row = [i, d['exercise'], d['sets']]\n",
    "            for param in param_list:\n",
    "                row.append(d.get(param, None))\n",
    "            flat_data.append(row)\n",
    "    columns = ['block', 'exercise', 'sets'] + param_list\n",
    "    \n",
    "    df = pd.DataFrame(flat_data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_outputs = [final_json_data[i]['output'] for i in range(len(final_json_data))]\n",
    "main_dataset_df = output_list_to_df(data_outputs,param_list)\n",
    "main_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_of_ex = main_dataset_df['exercise'].count()\n",
    "max_number_of_ex_per_block = main_dataset_df.groupby('block')['exercise'].count().max()\n",
    "average_number_of_ex_per_outer_list = main_dataset_df.groupby('block')['exercise'].count().mean()\n",
    "median_number_of_ex_per_outer_list = main_dataset_df.groupby('block')['exercise'].count().median()\n",
    "std_dev_of_ex_per_outer_list = main_dataset_df.groupby('block')['exercise'].count().std()\n",
    "number_of_blocks_with_one_exercise = (main_dataset_df.groupby('block')['exercise'].count() == 1).sum()\n",
    "\n",
    "\n",
    "print(f\"Total number of exercises: {total_number_of_ex}\")\n",
    "print(f\"Max number of exercises per block: {max_number_of_ex_per_block}\")\n",
    "print(f\"Average number of exercises per block: {average_number_of_ex_per_outer_list}\")\n",
    "print(f\"Median number of exercises per block: {median_number_of_ex_per_outer_list}\")\n",
    "print(f\"Standard deviation of exercises per block: {std_dev_of_ex_per_outer_list}\")\n",
    "print(f\"Percentage of blocks with only one exercise: {number_of_blocks_with_one_exercise/len(final_json_data) *100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sets = main_dataset_df['sets'].sum()\n",
    "max_sets_per_block = main_dataset_df.groupby('block')['sets'].sum().max()\n",
    "average_sets_per_outer_list = main_dataset_df.groupby('block')['sets'].sum().mean()\n",
    "median_sets_per_outer_list = main_dataset_df.groupby('block')['sets'].sum().median()\n",
    "std_dev_sets_per_outer_list = main_dataset_df.groupby('block')['sets'].sum().std()\n",
    "\n",
    "print(f\"Total number of sets: {total_sets}\")\n",
    "print(f\"Max number of sets per block: {max_sets_per_block}\")\n",
    "print(f\"Average number of sets per block: {average_sets_per_outer_list}\")\n",
    "print(f\"Median number of sets per block: {median_sets_per_outer_list}\")\n",
    "print(f\"Standard deviation of sets per block: {std_dev_sets_per_outer_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
